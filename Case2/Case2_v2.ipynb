{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabel of Contents:\n",
    "* [1 Case 2. Pneumonia X-ray image analysis](#case-2)\n",
    "* [2 Background](#case-background)\n",
    "* [3 Data](#loading-data)\n",
    "* [4 Exploratory Data Analysis And Preprocessing](#exploratory-analysis)\n",
    "* [5 Models and Training](#models-trainning)\n",
    "* [6 Conclusions](#cas-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2. Pneumonia X-ray image analysis\n",
    " <a class=\"anchor\" id=\"case-2\"></a>\n",
    "Team 14:<br>\n",
    "* Awet Ghebreslassie\n",
    "* Leevi Pelkonen\n",
    "* Visa Soininen<br><br>\n",
    "Last edited: 01.03.2020<br>\n",
    "Neural Networks for Health Technology Applications<br>\n",
    "[Helsinki Metropolia University of Applied Sciences](http://www.metropolia.fi/en/)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Background  <a class=\"anchor\" id=\"case-background\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to predict whether a person is sufferring from a lung-condition called Pneumonia. The early-phase or minor pneumonia can be hard to spot by looking at a chest-xray with the naked eye. Severe cases show clear fogginess in the lung area.\n",
    "\n",
    "In this notebook we use Convolutional Neural Networks (CNN) to determine if a person has pneumonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data  <a class=\"anchor\" id=\"loading-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided by Daniel Kermany, Kang Zhang and Michael Goldbaum from the University of California San Diego. The dataset contains chest x-ray images with persons suffering from pneunomia and healthy persons. The pictures are labeled and split into different sets if the lung appearing in the picture has pneumonia or not. Latest version of the dataset is published in 2018 and it is provided under the CC BY 4.0 -License.\n",
    "\n",
    "To optimize the training and prediction time of the CNN, the pictures are downscaled from the original size.s) (Value 0 = < 50% diameter narrowing; Value 1 = > 50% diameter narrowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%pylab inline\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, roc_curve, confusion_matrix\n",
    "from tensorflow.keras.metrics import SensitivityAtSpecificity\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = \"ChestXRay2017/chest_xray/train\"\n",
    "train_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/train'\n",
    "test_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/test'\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Exploratory Data Analysis And Preprocessing <a class=\"anchor\" id=\"exploratory-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, validation_split = 0.3)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "target_size = (158, 158)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = target_size,\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'binary',\n",
    "    subset = 'training')\n",
    "    \n",
    "dev_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = target_size,\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'binary',\n",
    "    subset = 'validation')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size= target_size,\n",
    "    batch_size= batch_size, \n",
    "    class_mode='binary',\n",
    "    shuffle= False)\n",
    "\n",
    "test_labels = test_generator.classes\n",
    "num_test_samples = len(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape', labels_batch.shape)\n",
    "    i = i + 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(data_batch[0])\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Models and training  <a class=\"anchor\" id=\"models-trainning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Plots accuracy and loss from model history\n",
    "    '''\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    sensitivity_specificity = history.history['sensitivity_at_specificity']\n",
    "    val_sensitivity_specificity = history.history['val_sensitivity_at_specificity']\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    plot(epochs, acc, 'b', label = 'Training acc')\n",
    "    plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "    title('Training and validation accuracy')\n",
    "    grid()\n",
    "    legend()\n",
    "\n",
    "    figure()\n",
    "    plot(epochs, loss, 'b', label = 'Training loss')\n",
    "    plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "    title('Training and validation loss')\n",
    "    grid()\n",
    "    legend()\n",
    "    \n",
    "    figure()\n",
    "    plot(epochs, sensitivity_specificity, 'b', label = 'Training sensitivity specificity')\n",
    "    plot(epochs, val_sensitivity_specificity, 'r', label = 'Validation sensitivity specificity')\n",
    "    title('Training and validation sensitivity specificity')\n",
    "    grid()\n",
    "    legend()\n",
    "\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_metrics(pred):\n",
    "    print('Confusion matrix\\n', confusion_matrix(test_labels, pred > 0.4).T)\n",
    "    print('\\nClassification report\\n', classification_report(test_labels, pred > 0.4, target_names = ['Normal(0)', 'Pneumonia(1)']))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, pred, pos_label = 1)\n",
    "    plot(fpr, tpr)\n",
    "    plot([0, 1], [0, 1], 'r:')\n",
    "    xlabel('False positive rate')\n",
    "    ylabel('True positive rate')\n",
    "    title('ROC curve')\n",
    "    xlim([0, 1])\n",
    "    ylim([0, 1])\n",
    "    grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [ SensitivityAtSpecificity(0.9, name= 'sensitivity_at_specificity'), 'acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (158, 158, 3)))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "model_1.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "model_1.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "model_1.add(layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "model_1.add(layers.Flatten())\n",
    "model_1.add(layers.Dense(512, activation = 'relu'))\n",
    "model_1.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr = 1e-4), metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the model\n",
    "history_1 = model_1.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 10,\n",
    "    verbose = 0,\n",
    "    epochs = 30,\n",
    "    validation_data = dev_generator,\n",
    "    validation_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecting using test data\n",
    "pred_1 = model_1.predict_generator(test_generator, steps=num_test_samples/batch_size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_metrics(pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "In this model we will experiment with pretrained model VGG16 downloaded from imagenet, we used [this](https://medium.com/unit8-machine-learning-publication/detecting-pneumonia-on-x-ray-images-covnets-and-transfer-learning-6d94b58c6657) articel as reference to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(158, 158, 3))\n",
    "flattened_output = layers.Flatten()(base_model.output)\n",
    "final_output = layers.Dense(1, activation='sigmoid')(flattened_output)\n",
    "\n",
    "model_2 = models.Model(inputs= base_model.input, outputs= final_output)\n",
    "\n",
    "# to reduce trainable paramters set trainable to false for the first 20 layers (upto Flatten later)\n",
    "for layer in model_2.layers[0:20]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss = 'binary_crossentropy', optimizer = optimizers.Adam(), metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the model\n",
    "history_2 = model_2.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 10,\n",
    "    verbose = 0,\n",
    "    epochs = 10,\n",
    "    validation_data = dev_generator,\n",
    "    validation_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecting using test data\n",
    "pred_2 = model_2.predict_generator(test_generator, steps=num_test_samples/batch_size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_metrics(pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "\n",
    "In this model we tried to implement LeNet-5 classical NNN architecture using [this](https://medium.com/@mgazar/lenet-5-in-9-lines-of-code-using-keras-ac99294c8086) articel as reference to test how it would perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = models.Sequential()\n",
    "model_3.add(layers.Conv2D(6, (3, 3), activation='relu', input_shape=(158, 158, 3)))\n",
    "model_3.add(layers.AveragePooling2D())\n",
    "model_3.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
    "model_3.add(layers.AveragePooling2D())\n",
    "model_3.add(layers.Flatten())\n",
    "model_3.add(layers.Dense(20, activation='relu'))\n",
    "model_3.add(layers.Dense(84, activation='relu'))\n",
    "model_3.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss = 'binary_crossentropy', optimizer = optimizers.Adam(), metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the model\n",
    "history_3 = model_3.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = 10,\n",
    "    verbose = 0,\n",
    "    epochs = 10,\n",
    "    validation_data = dev_generator,\n",
    "    validation_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecting using test data\n",
    "pred_3 = model_3.predict_generator(test_generator, steps=num_test_samples/batch_size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_metrics(pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Conclusions  <a class=\"anchor\" id=\"cas-conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some very intersting results from our experiment, something that was common to all our models was that they performed unxpexpectedley worse on the test data set than the validation data set, this is unexpected since the validation dataset is not used in training the models. Overall we were able to create different models with acceptable performance.\n",
    "Best perfoming model was Model2 which was construced using pre-trained network called VGG16. The reason Model2 is our winner model is because it have better accuracy and reduced time and computational power and better overall consistent high accuracy, the accuracy of the models change with each re-run due to random selection of the images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
